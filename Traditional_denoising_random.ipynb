{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jDdtUejI85QB"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lY7juHsJ85QF"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "VAL_SIZE = 0.2\n",
    "AMNT_BATCHS = 400\n",
    "DATASET_SIZE = AMNT_BATCHS * BATCH_SIZE\n",
    "EPOCHS = 40\n",
    "NRO_MODELO = \"modelo_NoAERandomPosta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainImages = keras.utils.image_dataset_from_directory(\n",
    "    'places/',\n",
    "    label_mode=None,\n",
    "    image_size=(256,256),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False).take(AMNT_BATCHS)\n",
    "\n",
    "trainImages = trainImages.map( lambda x : x / 255 )\n",
    "\n",
    "blurredTrainImages = keras.utils.image_dataset_from_directory(\n",
    "    'blurredRandomPlaces/',\n",
    "    label_mode=None,\n",
    "    image_size=(256,256),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False).take(AMNT_BATCHS)\n",
    "\n",
    "blurredTrainImages = blurredTrainImages.map( lambda x : x / 255 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "id": "y4orlt-1JCKK",
    "outputId": "cb71b4be-f774-47ed-ced7-a03b547b53fb"
   },
   "outputs": [],
   "source": [
    "valSplit = int( (DATASET_SIZE / BATCH_SIZE) * VAL_SIZE )\n",
    "\n",
    "train =  tf.data.Dataset.zip((blurredTrainImages, trainImages))\n",
    "\n",
    "datasetTrain = train.skip(valSplit)\n",
    "datasetValidation = train.take(valSplit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_mQiHAwD85QI",
    "outputId": "5b17c7b6-0bf5-4b69-9aaa-9313673475cc"
   },
   "outputs": [],
   "source": [
    "input = layers.Input(shape=(256, 256, 3))\n",
    "\n",
    "# Encoder\n",
    "x = layers.Conv2D(32, (5, 5), activation=\"relu\", padding=\"same\")(input)\n",
    "x = layers.Conv2D(32, (5, 5), activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.Conv2D(32, (5, 5), activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.Conv2D(32, (5, 5), activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.Conv2D(32, (5, 5), activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.Conv2D(32, (5, 5), activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.Conv2D(32, (5, 5), activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.Conv2D(32, (5, 5), activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.Conv2D(32, (5, 5), activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.Conv2D(32, (5, 5), activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.Conv2D(3, (5, 5), activation=\"sigmoid\", padding=\"same\")(x)\n",
    "\n",
    "# Autoencoder\n",
    "autoencoder = Model(input, x)\n",
    "autoencoder.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
    "\n",
    "modelFile = open('checkpoints/' + NRO_MODELO + '/modelo.txt', 'w+')\n",
    "autoencoder.summary(print_fn=lambda x: modelFile.write(x + '\\n'))\n",
    "modelFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeLog( file, epoch, log ):\n",
    "    file.write(str(epoch) + ',' + str(log['loss'])+ ',' + str(log['val_loss']) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "acKLqAZ6bLa2",
    "outputId": "8008a7a1-2106-4394-8769-53536783461b"
   },
   "outputs": [],
   "source": [
    "modelCallback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath='checkpoints/' + NRO_MODELO + '/model.ckpt',\n",
    "        save_weights_only=True,\n",
    "        verbose=1)\n",
    "\n",
    "lossCkptFile = open('checkpoints/' + NRO_MODELO + '/loss.ckpt', 'a+')\n",
    "lossCallback = keras.callbacks.LambdaCallback(on_epoch_end=lambda epoch, logs: writeLog( lossCkptFile, epoch, logs))\n",
    "\n",
    "historia = autoencoder.fit(\n",
    "    datasetTrain,\n",
    "    epochs=EPOCHS,\n",
    "    shuffle=True,\n",
    "    validation_data=datasetValidation,\n",
    "    callbacks = [modelCallback, lossCallback]\n",
    ")\n",
    "\n",
    "lossCkptFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "id": "RrIaNxkh85QL",
    "outputId": "18562100-d029-47df-c2ea-8fd03a5cb933"
   },
   "outputs": [],
   "source": [
    "predictions = autoencoder.predict( datasetValidation.take(1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotPrediccion( entrada, prediccion ):\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    for i, (image1, image2) in enumerate(zip(entrada, prediccion)):\n",
    "        ax = plt.subplot(2, 1, i + 1)\n",
    "        image1Np = image1[0].numpy()\n",
    "        #clip01 = lambda x : (x + np.ones_like(x) * np.abs( x.min() ) ) / (x + np.ones_like(x) * np.abs( x.min() ) ).max()\n",
    "        \n",
    "        plt.imshow( image1Np[0] )\n",
    "\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        ax = plt.subplot(2, 1, i + 2)\n",
    "\n",
    "        plt.imshow( image2 )\n",
    "\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotPrediccion( datasetValidation.take(1), predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(historia.history['loss'])\n",
    "plt.plot(historia.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Autoencoder denoising",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.5 ('learning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4dfce521a65dafdb7fa4a85c9590d08c96b241eed376b791f905cb7e0f811ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
